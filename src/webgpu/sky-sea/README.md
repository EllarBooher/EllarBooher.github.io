# WebGPU Sky and Sea

## Abbreviations

- FT - fourier transform
- FFT - fast fourier transform
- IFFT - inverse fast fourier transform
- LUT - lookup table
- BRDF - bidirectional reflectance distribution function

## Features

- Ocean
  - Gerstner waves, also known as trochoid waves
  - FFT acceleration for millions of waves instead of a few dozen
  - Gravity-waves with directional spreading and JONSWAP spectrum
  - Parameterized wind
  - Cascades that subdivide full frequency spectrum for more detail
  - Foam that forms and dissipates based on surface jacobian
  - Ocean surface mesh projected from screen-space to world-space to minimize offscreen geometry
  - Rasterization into GBuffer with per vertex displacement and per fragment normals
- Atmosphere
  - Dynamic time of day
  - Scattering integral evaluated via raymarching
  - Rayleigh, Mie, and Ozone scattering
  - Multiple scattering and aerial perspective
  - Composited sun disk
  - Lookup tables for transmittance, view of the sky, multiscattering, and aerial perspective
- Ocean surface shading includes luminance from the sky and sun
- Able to display most intermediate resources (such as the complex amplitudes of the ocean FFT spectrum) to the screen, see "Render Output > Render Output" in the controls

## About

The renderer works by generating the atmosphere and ocean, before compositing them in a final compute pass.

### Ocean

The ocean surface is a mesh whose vertices are generated on the GPU and then rasterized into a GBuffer. The GBuffer contains textures for color, world-space normals, and depth. World-space distance is packed into the alpha channel of the color texture, and foam strength is packed into the alpha channel of the normal texture.

See [`wave_surface_displacement.wgsl`](../../shaders/sky-sea/ocean/wave_surface_displacement.wgsl) and [`WaveDisplacement.ts`](./ocean/WaveDisplacement.ts) for the implementation of the ocean surface mesh. The mesh vertices are first generated by projecting from the screen-space of the scene camera onto the spherical planet surface. This gives us a subdivided quad that is the rough intersection of the camera frustum with the unperturbed ocean, with near constant screen-space vertex density. Even though the eventual displacement due to waves warps the density, this means we are dedicating roughly the same amount of computation time for every rasterized pixel.

Once we have these initial vertices, we displace each vertex to simulate waves. The technique for calculating the displacements and surface normals are based primarily on the 2001 paper "Simulating Ocean Water" by Jerry Tessendorf, see [[1]](#tessendorf-2001) for all the details. The master's thesis "Ocean surface generation and rendering" by Thomas Gamper [[5]](#gamper-2018) provides an excellent, more modern survey of ocean simulation. We'll give an overview here.

The per-vertex displacement is three dimensional, since this allows for more realistic [trochoidal waves](https://en.wikipedia.org/wiki/Trochoidal_wave) that "roll" like real ocean waves. A formulation goes as follows:

$$
\begin{align*}
t &: \text{time in seconds} \\
\vec r = (x,y,z) &: \text{position of an ocean particle in meters} \\
\vec k = (k_x,k_z) &: \text{angular wave vector in radians per meter} \\
\omega &: \text{angular frequency in radians per second} \\
\vec D(\vec r,t) = (D_x,D_y,D_z) &: \text{displacement of a given ocean particle} \\
A &: \text{amplitude of the wave in meters}
\end{align*}
$$

So if we identify our waves by a unique wave vector $\vec k$ and parameters $A(\vec k)$ and $\omega(\vec k)$, we have:

$$
\begin{align*}
\theta(\vec k,\vec r,t) &= \vec k \cdot \vec r - \omega(\vec k) t \\
\hat k &= \frac{\vec k}{\left\|\vec k\right\|} = (\hat k_x, \hat k_z)\\
\vec D(\vec k,\vec r,t) &= \sum_{\vec k} \left(-A\hat k_x \sin(\theta),A\cos(\theta),-A\hat k_z \sin(\theta)\right)
\end{align*}
$$

Note that $\hat k$ gives the direction that the wave propagates, so this formulation describes ocean surface particles as moving in a circle parallel to the direction of travel. The wave parameters can be loaded from a uniform buffer of waves. This works for stylistic oceans, but for hundreds of waves the result can be unrealistic. You could use millions of waves, but the linear scaling of the sum leads to poor performance.

Luckily, we can apply the Fast Fourier Transform (FFT) to knock down the complexity, since we can reformulate our sum to be a Fourier transform. See [`fourier_waves.wgsl`](../../shaders/sky-sea/ocean/fourier_waves.wgsl) and [`FourierWaves.ts`](./ocean/FourierWaves.ts) for the creation of the complex-valued wave spectrum, or [`fft.wgsl`](../../shaders/sky-sea/util/fft.wgsl) and [`FFT.ts`](./util/FFT.ts) for the general 2D FFT implementation. We reformulate our spectrum of waves from a linear list to a square grid of width $n$, and our displacement map shall be the same dimension and size. This is the primary restriction of the periodic DFFT: each sample in the frequency domain gives one sample in the spatial domain. In other words, $n^2$ waves gives $n^2$ unique displacement in our texture. This fact is not necessarily a negative, but we do need to design around it.

Calculating a single vertex displacement on this grid as a naive sum is $O(n^2)$. However, a normal FFT has $O(n \log n)$ time complexity, allowing for many more waves. Each component of the displacement requires its own inverse FFT to compute, so we require three inverse FFTs, which mirrors the three sine or cosine calculations per wave we saw earlier. The wave spectrum formulation is:

$$
\begin{align*}
 \tilde{h}(k,t) &: \text{frequency domain amplitude}\\
\vec{h}(\vec k,t) &= \tilde{h}(k,t)\left(-i\hat{k}_x,1,-i\hat{k}_z\right) \\
\vec D(\vec r,t) &= \sum_{\vec k} \vec h(\vec k,t)e^{i\vec k \cdot \vec r} \\
\end{align*}
$$

The per-wave complex-valued amplitude $\tilde{h}(\vec k,t)$, analogous to the real-valued $A$, is calculated from empirically derived ocean spectrum models such as JONSWAP. See Tessendorf and Gamper's papers (cited earlier) for more discussion, since there is a lot of freedom for what to use depending on which ocean conditions you wish to accurately portray. One important criteria is that $\tilde{h}$ is [Hermitian](https://en.wikipedia.org/wiki/Hermitian_function), which results in the Fourier transform being real-valued. In the end, we compute these amplitudes with a mixture of random noise and parameters such as wind speed, wind fetch, simulation time, and others.

We also use three cascades to divide the ocean spectrum and generate three displacement maps at different world-space scales. This allows us to capture detail across multiple scales, since for good performance the grid size of the ocean spectrum must be quite small. We chose 512 by 512 waves for each cascade, with scales of 200, 50, and 10 meters. This is $512 * 512 * 3 = 786432$ unique waves in total, although many are low contribution. This leads to a spatial and wavelength sample rate of $10 / 512 \approx 0.02$ meters, which is roughly two centimeters. This is the approximate boundary at which gravity waves become capillary waves and surface tension becomes the dominating force. The models we use rely on the fact that gravity dominates for larger wavelengths, and do not sufficiently account for surface tension.

Next, surface normals are needed for shading. We calculate these from the gradients of the displacement, instead of using a numerical method like finite differences. We compute the tangent, bitangent, and normal from the final displaced ocean surface vertex position $\vec D(\vec r,t)$ as follows:

$$
\begin{align*}
\vec P(\vec r,t) &\coloneqq \text{Final ocean surface particle position}\\
\vec P(\vec r,t) &= \vec r + \vec D(\vec r,t) = \left(x+\vec D_x,y+\vec D_y,z+\vec D_z\right)\\
\vec T &= \frac{d}{dx}\vec P(\vec r,t) = \left(1+\frac{d}{dx}\vec D_x,\frac{d}{dx}\vec D_y,\frac{d}{dx}\vec D_z\right)\\
\vec B &= \frac{d}{dz}\vec P(\vec r,t) = \left(\frac{d}{dz}\vec D_x,\frac{d}{dz}\vec D_y,1+\frac{d}{dz}\vec D_z\right)\\
\vec N &= \vec T \times \vec B \\
\end{align*}
$$

The discrete fourier transform is a sum, so the derivative distributes over it. Thus we can instead take the inverse fourier of the per-term partial derivatives. We need six partial derivatives obtained by distributing $\frac{d}{dx}$ and $\frac{d}{dz}$ to the three components of the displacement. It can be shown that the mixed partials $\frac{d}{dx}D_z$ and $\frac{d}{dz}D_x$ are equal, so we only need to compute five extra IFFTs for eight in total. A single derivative of the DFFT goes as follows:

$$
\begin{align*}
x_i &\in {x,z} \\
\frac{d}{dx_i}\vec D_{x_i}(\vec k,\vec r,t) &= \frac{d}{dx_i}\sum_{\vec k} \vec h(\vec k,t)e^{i\vec k \cdot \vec r} \\
&= \sum_{\vec k} \vec h(\vec k,t)\frac{d}{dx_i}e^{i\vec k \cdot \vec r} \\
&= \sum_{\vec k} ik_{x_i}\vec h(\vec k,t)e^{i\vec k \cdot \vec r}
\end{align*}
$$

We can improve the performance further. Since the displacement is real-valued, we can pack two IFFTs into one by summing two streams of input data while multiplying one by $i$. The discrete fourier transform is a linear sum, so multiplying the inputs by a scalar multiplies the output by that same scalar. For example:

$$
\tilde{h}_x(k,t)+i\tilde{h}_z(k,t)\xmapsto{IFFT}\vec D_x+i\vec D_z
$$

From this we can easily extract $\vec D_x$ and $\vec D_z$ since they will be in separate vector components. Furthermore, since textures have four components, we can pack two IFFTs into one set of dispatches via concatenation. See `computeTimeDependentAmplitude` of [`fourier_waves.wgsl`](../../shaders/sky-sea/ocean/fourier_waves.wgsl#L354) for how we pack the spectrum data.

In the end, we write the displacement, partial derivatives, and surface jacobian into arrays of maps that are sampled in the vertex and fragment shaders to rasterize the final ocean surface mesh into the GBuffer.

### Atmosphere

The technique for the atmosphere is taken from a 2020 paper by Sébastien Hillaire titled "A scalable and production ready sky and atmosphere rendering technique" (see [[6]](#hillaire-2020)). The final atmospheric output is a set of four lookup tables: the transmittance LUT, skyview LUT, multiscatter LUT, and aerial perspective LUT. See Hillaire's paper for all of the equations, but here is a brief overview of each resource. The parameterization of these maps is based on the work in [[2]](#bruneton-2008), which Hillaire's paper also cites.

The atmosphere is modelled as a medium that continuously scatters light, shaped as a spherical shell around a spherical planet with a sharp boundary. We consider scattering and absorption effects from Mie, Rayleigh, and Ozone particles. See `computeLuminanceScatteringIntegral` in [`atmosphere_raymarch.inc.wgsl`](../../shaders/sky-sea/atmosphere/atmosphere_raymarch.inc.wgsl) for the raymarching algorithm used to evaluate the scattering integral, which is used whenever we need to compute primary scattered light along a ray. Per-atmospheric-position scattering and absorption determine [optical depth](https://en.wikipedia.org/wiki/Optical_depth) along each raymarched path segment, see [`GlobalUBO.ts`](./GlobalUBO.ts) and `sampleExtinction` in [`atmosphere_common.inc.wgsl`](../../shaders/sky-sea/atmosphere/atmosphere_common.inc.wgsl). Mie and Rayleigh extinction is modelled with exponential density functions that decay with altitude as a parameter, with maximal density at the surface. Ozone extinction is modelled with a tent function that places its maximal influence much higher in the atmosphere.

The transmittance LUT (see [`transmittance_LUT.wgsl`](../../shaders/sky-sea/atmosphere/transmittance_LUT.wgsl)) measures the spectral transmittance for red, blue, and green along a given view ray, all the way to the edge of the atmosphere. The map is parameterized by altitude and view zenith angle. The transmittance is found by integrating optical depth along a discrete number of steps, where a smaller step size leads to higher accuracy. Sampling the transmittance LUT should be avoided for low altitudes at angles nearly parallel to the surface of the planet, since you run into precision issues. In general, raymarching to determine transmittance is more accurate but the LUT is faster. By default, we recompute transmittance when raymarching luminance, which we only do in our lookup tables and not in the final composition pass.

The multiscatter LUT (see [`multiscatter_LUT.wgsl`](../../shaders/sky-sea/atmosphere/multiscatter_LUT.wgsl)) contains a map of the total incoming luminance from all directions due to higher orders of scattering, in other words light that bounces a few times. Due to the symmetry of a spherical earth, the map is parameterized by altitude and light zenith angle. Since time of day is constant during a frame, during any given raymarch a single 1D slice of this map is "active", and the raymarch samples along the slice. Higher orders of scattered light are roughly correlated, which means their sum can be accurately estimated by a geometric series, making the multiscattering LUT much easier to calculate. Even so, this calculation is the most expensive of our LUTs.

The skyview LUT (see [`skyview_LUT.wgsl`](../../shaders/sky-sea/atmosphere/skyview_LUT.wgsl)) contains a map of the total incoming luminance along a ray from the camera's position. Each texel is a single evaluation of the scattering integral for light reaching the observer. It contains a projection of the full spherical view from the camera, and is parameterized by azimuth and zenith angles of a given view direction. This LUT provides a short computation path for unobstructed views of the sky, and can be a relatively low resolution due to the low signal frequency of the luminance.

The aerial perspective LUT (see [`aerial_perspective_LUT.wgsl`](../../shaders/sky-sea/atmosphere/aerial_perspective_LUT.wgsl)) is used for determining the [aerial perspective](https://en.wikipedia.org/wiki/Aerial_perspective) for surface geometry, in which the increasing atmospheric distance between the viewer and a distant object accumulates scattered light. The aerial perspective LUT is fit to the camera frustum, and is an array with multiple depth slices. By default the texture is 32 texels on all dimensions, with a range of one kilometer per depth-slice. Aerial perspective is low frequency, so this small resolution is suitable. The aerial perspective LUT is very similar to the skyview LUT, just with different parameterization.

Another important detail of the aerial perspective LUT is that we pack wavelength averaged transmittance into the alpha channel, using the transmittance calculated while raymarching the scattering. As mentioned for the transmittance LUT, there are floating point precision issues near the surface. By using the raymarched transmittance from the aerial perspective LUT, we avoid these issues. This is important for pixels near the horizon, since otherwise you get incorrect color ratios and possibly banding. We could store full spectral transmittance in a separate texture, but that is not necessary to get good enough results.

Each frame, we only need to recompute the skyview LUT and aerial perspective LUT, since those two are the ones that depend on time of day and viewer position. The transmittance LUT and multiscatter LUT only depend on the atmospheric medium's parameters, and thus only need to be regenerated sparingly.

### Composition

See [`atmosphere_camera.wgsl`](../../shaders/sky-sea/atmosphere_camera.wgsl) and [`AtmosphereCamera.ts`](./AtmosphereCamera.ts) for where the rendering of the final scene occurs.

We start with the ocean surface in the GBuffer. For each texel, we decide if it contains surface geometry based on if the world distance was set in the GBuffer. If it contains the sky, we sample the skyview LUT and layer a sun disk on top. If it contains geometry, we assume it is water and shade it accordingly. The model we use for shading the ocean pixels is quite simple and inaccurate, since the focus of this project was on the ocean simulation and general integration with the atmosphere.

We wish to compute the luminance incident to the camera for every pixel. Consider:

$$
\begin{align*}
\vec C &: \text{Camera position} \\
\vec P &: \text{Surface position} \\
\vec L &: \text{Light direction outgoing from surface} \\
\vec V &\coloneqq \operatorname{normalize}(\vec C - \vec P) \text{, View direction} \\
\vec H &\coloneqq \operatorname{normalize(\vec L + \vec V)} \text{, Halfway vector} \\
\vec N &: \text{Surface normal} \\
R(\vec v,\vec n) &: \text{Fresnel factor for direction $\vec v$ and surface normal $\vec n$} \\
L_{out}(\vec p,\vec v) &: \text{Outgoing luminance from position $\vec p$ to direction $\vec v$ } \\
L_{in}(\vec p,\vec v) &: \text{Incoming luminance to position $\vec p$ from direction $\vec v$} \\
T(\vec x, \vec y) &: \text{Transmittance between positions $\vec x$ and $\vec y$} \\
\operatorname{BRDF}(\vec a, \vec b) &: \text{BRDF for outgoing directions $\vec a$ and $\vec b$} \\
\omega &: \text{Solid angle in steradians} \\
\end{align*}
$$

We wish to compute $L_{in}(\vec C, -\vec V)$, the total luminance reaching the camera in the direction of the surface. We split it up into contributions from the scattered sky dome and un-scattered sun as follows:

$$
\begin{align*}
L_{in}(\vec C, -\vec V) &= T(\vec C, \vec P)*L_{out}(\vec P, \vec V) \\
L_{out}(\vec P,\vec V) &= L_{sky}+L_{sun} \\
\end{align*}
$$

Estimating the luminance from the sky dome accurately is difficult, see [[3]](#bruneton-2010) for some techniques addressing this. We take a simple approach and split into separate specular and diffuse terms, with disjoint sampling strategies. The samples chosen are arbitrary and imprecise, but they work well enough for a visually pleasing scene.

$$
\begin{align*}
L_{sky} &= \omega_s*L_{in,sky}(\vec C,\vec v_s)*\operatorname{BRDF_s}(\vec v_s,\vec V)R_s  \\
&+ \omega_d*L_{in,sky}(\vec C,\vec v_d)*\operatorname{BRDF_d}(\vec v_d,\vec V)R_d \\
\omega_s &= \frac{4\pi}{200}\\
\vec v_{s} &= \operatorname{reflect(\vec V, \vec N)} \\
R_s &= R(\vec v_s, \vec N) \\
\omega_d &= 2\pi \\
\vec v_{d} &= \operatorname{normalize(\vec L+(0,1,0))} \\
R_d &= 1 - R(\vec v_d, \operatorname{normalize}(\vec v_d+\vec L)) \\
\end{align*}
$$

$L_{in,sky}$ is sampled from the skyview LUT. Since the skyview LUT is dependent on the cameras position, this could be inaccurate for distant $\vec P$ near the horizon. However, there are not too many such pixels and they are typically obscured by aerial perspective anyway.

For $BRDF_s$, we use a specular microfacet BRDF. To determine the sample, we assume the maximal impact sample is in the direction of a perfect reflection. At sunset, the luminance varies by about 20 times from minimum to maximum. At noon, this variation is only 2 times. Thus, since the specular BRDF has a sharp falloff, this assumption is reasonably accurate. So our choice of $\vec v_s$ in the direction of a perfect reflection is suitable.

For $BRDF_d$, we use a diffuse lambertian BRDF. To determine the sample, we assume a sample direction that is halfway between the light and world up roughly approximates the mean luminance from the sky. This is because, as stated earlier for the specular sample, the sky luminance does not vary much. So our choice of $\vec v_d$ is suitable.

We compute the sun luminance as follows, where $E_{sun}$ is the illuminance from the sun incident to the atmospheric boundary:

$$
\begin{align*}
S(\vec p, \vec l) \in [0,1] &: \text{Sun visibility at surface position $\vec p$ and light direction $\vec l$} \\
\vec A(\vec o, \vec d) &: \text{Atmosphere boundary intersection point from $\vec o$ in direction $\vec d$} \\
L_{sun} &= S(\vec C, \vec L) * T(\vec P, \vec A(\vec C, \vec L)) * E_{sun} * BRDF_{sun} \\
BRDF_{sun} &= \operatorname{lerp}(BRDF_s(\vec L, \vec V),BRDF_d(\vec L, \vec V),\vec R(\vec L,\vec H))\\
\end{align*}
$$

The visibility of the sun varies as the sun dips below the horizon, and thus so does the incoming luminance.

An important fact that is not obvious unless you examine the rendering equations we use is that our luminance factors linearly:

$$
L_{in}(\vec C, -\vec V) = L_{transfer}(\vec C, -\vec V)E_{sun}
$$

What we have actually calculated this entire time is $L_{transfer}$. We have the freedom to choose $E_{sun} = 1$ and ignore it in all lighting calculations. Then as the final step before presentation, we multiply by an arbitrary solar strength parameter that combines the sun's apparent solid angle and luminous intensity.

After computing $L_{in}(\vec C, -\vec V)$, we multiply it by the strength and color of the sun to get the final HDR luminance. We convert to sRGB with the ACES tonemapping function, then present to the screen.

## Further Work

- Improve the tiling to make it aperiodic, see [[7]](#lutz-2024) for an option for the technique
- Better level-of-detail for the distant ocean
  - Currently displacement maps are sampled with mipmaps and anisotropic filtering, this still leads to instability when the camera moves due to the unstable vertex positions of the ocean surface mesh
  - The distant ocean needs to be modelled in a way that captures the variance of the slopes, potentially see the BRDF technique by Bruneton et. al. in [[3]](#bruneton-2010)
- The ocean surface needs a more realistic lighting model. It is currently lacking:
  - Light transmitting through waves
  - Secondary bounces
  - Refraction
  - Accurate estimation of sky dome luminance
- More realistic foam and spray
  - Handle loops in wave crests
- Underwater camera
- Clouds
- Volumetric shadows
- Moon
- Smooth the sun-disk

## Sources

<a class="citation" id="tessendorf-2001">[1]</a>
Jerry Tessendorf. 2001.
Simulating Ocean Water.
SIG-GRAPH'99 Course Note.
<https://jtessen.people.clemson.edu/reports/papers_files/coursenotes2002.pdf>

<a class="citation" id="bruneton-2008">[2]</a>
Eric Bruneton, Fabrice Neyret. 2008.
Precomputed Atmospheric Scattering.
Computer Graphics Forum, Special Issue: Proceedings of the 19th Eurographics Symposium on Rendering 2008, 27(4), pp.1079-1086.
<https://doi.org/10.1111/j.1467-8659.2008.01245.x> . <https://inria.hal.science/inria-00288758>

<a class="citation" id="bruneton-2010">[3]</a>
Eric Bruneton, Fabrice Neyret, Nicolas Holzschuch. 2010.
Real-time Realistic Ocean Lighting using Seamless Transitions from Geometry to BRDF.
Computer Graphics Forum, 29(2), pp. 487-496.
<https://doi.org/10.1111/j.1467-8659.2009.01618.x> . <https://inria.hal.science/inria-00443630v3>

<a class="citation" id="flugge-2017">[4]</a>
Fynn-Jorin Flügge. 2017.
Realtime GPGPU FFT ocean water simulation.
<https://doi.org/10.15480/882.1436>

<a class="citation" id="gamper-2018">[5]</a>
Thomas Gamper. 2018.
Ocean surface generation and rendering [Diploma Thesis, Technische Universität Wien].
reposiTUm.
<https://doi.org/10.34726/hss.2018.57880>

<a class="citation" id="hillaire-2020">[6]</a>
Sébastien Hillaire. 2020.
A scalable and production ready sky and atmosphere rendering technique.
Computer Graphics Forum, 39(4), pp. 13–22.
<https://doi.org/10.1111/cgf.14050>

<a class="citation" id="lutz-2024">[7]</a>
Nicolas Lutz, Arnaud Schoentgen, and Guillaume Gilet. 2024.
Fast orientable aperiodic ocean synthesis using tiling and blending.
Proc. ACM Comput. Graph. Interact. Tech. 7, 3, Article 49 (August 2024), 22 pages.
<https://doi.org/10.1145/3675388>
