# WebGPU Sky and Sea

## Abbreviations

- FT - fourier transform
- FFT - fast fourier transform
- IFFT - inverse fast fourier transform
- LUT - lookup table
- BRDF - bidirectional reflectance distribution function

## Features

- Ocean
  - Gerstner waves, also known as trochoid waves
  - FFT acceleration for millions of waves instead of a few dozen
  - Gravity-waves with directional spreading and JONSWAP spectrum
  - Parameterized wind
  - Cascades that subdivide full frequency spectrum for more detail
  - Foam that forms and dissipates based on surface jacobian
  - Ocean surface mesh projected from screen-space to world-space to minimize offscreen geometry
  - Rasterization into GBuffer with per vertex displacement and per fragment normals
- Atmosphere
  - Dynamic time of day
  - Scattering integral evaluated via raymarching
  - Rayleigh, Mie, and Ozone scattering
  - Multiple scattering and aerial perspective
  - Composited sun disk
  - Lookup tables for transmittance, view of the sky, multiscattering, and aerial perspective
- Ocean surface shading includes luminance from the sky and sun
- Able to display most intermediate resources (such as the complex amplitudes of the ocean FFT spectrum) to the screen, see "Render Output > Render Output" in the controls

## About

The renderer works by generating the atmosphere and ocean, before compositing them in a final compute pass.

### Ocean

The ocean surface is a mesh whose vertices are generated on the GPU and then rasterized into a GBuffer. The GBuffer contains textures for color, world-space normals, and depth. World-space distance is packed into the alpha channel of the color texture, and foam strength is packed into the alpha channel of the normal texture.

See [`wave_surface_displacement.wgsl`](../../shaders/sky-sea/ocean/wave_surface_displacement.wgsl) and [`WaveDisplacement.ts`](./ocean/WaveDisplacement.ts) for the implementation of the ocean surface mesh. The mesh vertices are first generated by projecting from the screen-space of the scene camera onto the spherical planet surface. This gives us a subdivided quad that is the rough intersection of the camera frustum with the unperturbed ocean, with near constant screen-space vertex density. Even though the eventual displacement due to waves warps the density, this means we are dedicating roughly the same amount of computation time for every rasterized pixel.

Once we have these initial vertices, we displace each vertex to simulate waves. The technique for calculating the displacements and surface normals are based primarily on the 2001 paper "Simulating Ocean Water" by Jerry Tessendorf, see [[1]](#tessendorf-2001) for all the details. The master's thesis "Ocean surface generation and rendering" by Thomas Gamper [[5]](#gamper-2018) provides an excellent, more modern survey of ocean simulation. We'll give an overview here.

The per-vertex displacement is three dimensional, since this allows for more realistic [trochoidal waves](https://en.wikipedia.org/wiki/Trochoidal_wave) that "roll" like real ocean waves. A formulation goes as follows:

$$
\begin{align*}
t &: \text{time in seconds} \\
\vec r = (x,z) &: \text{position of an ocean particle in meters} \\
\vec k = (k_x,k_z) &: \text{angular wave vector in radians per meter} \\
\omega &: \text{angular frequency in radians per second} \\
\vec D(\vec r,t) = (D_x,D_y,D_z) &: \text{displacement of a given ocean particle} \\
A &: \text{amplitude of the wave in meters}
\end{align*}
$$

So if we index waves by $j$, we can have:

$$
\begin{align*}
\theta &= \vec k \cdot \vec r - \omega t \\
D_{j,y} &= A\cos(\theta) \\
D_{j,x} &= -A\frac{k_x}{||\vec k||} \sin(\theta) \\
D_{j,z} &= -A\frac{k_z}{||\vec k||} \sin(\theta) \\
(D_x,D_y,D_z) &= \sum_{j=0}^N (D_{j,x},D_{j,y},D_{j,z})
\end{align*}
$$

Note that $\frac{\vec k}{||\vec k||}$ gives the direction that the wave propagates, so this formulation describes ocean surface particles as moving in a circle parallel to the direction of travel. The wave parameters can be loaded from a uniform buffer of waves. This works for stylistic oceans, but for hundreds of waves the result can be unrealistic. You could use millions of waves, but the `O(N)` scaling of the sum leads to poor performance.

Luckily, we can apply the Fast Fourier Transform (FFT) to knock down the complexity, since we can reformulate our sum to be a Fourier transform. See [`fourier_waves.wgsl`](../../shaders/sky-sea/ocean/fourier_waves.wgsl) and [`FourierWaves.ts`](./ocean/FourierWaves.ts) for the creation of the complex-valued wave spectrum, or [`fft.wgsl`](../../shaders/sky-sea/util/fft.wgsl) and [`FFT.ts`](./util/FFT.ts) for the general FFT implementation. We reformulate our spectrum of waves from a linear list to a square grid of width `n`, so calculating a single vertex displacement is naively an `O(n^2)` operation. A normal FFT has `O(n * log n)` time complexity, allowing for many more waves. Each component of the displacement requires its own inverse FFT to compute, so we require three inverse FFTs, which mirrors the three sine or cosine calculations per wave. The wave spectrum formulation is:

$$
\begin{align*}
 \tilde{h}(k,t) &: \text{frequency domain amplitude}\\
\vec{h}(\vec k,t) &= \tilde{h}(k,t)\left(-i\frac{k_x}{||\vec k||},1,-i\frac{k_z}{||\vec k||}\right) \\
\vec D(\vec r,t) &= \sum_{\vec k} \vec h(\vec k,t)e^{i\vec k \cdot \vec r} \\
\end{align*}
$$

The per-wave complex-valued amplitude $\tilde{h}(\vec k,t)$, analogous to the real-valued $A$, is calculated from empirically derived ocean spectrum models such as JONSWAP. See Tessendorf and Gamper's papers for more discussion, since there is a lot of freedom for what to use depending on which ocean conditions you wish to accurately portray. The only important condition is that $\tilde{h}$ is [Hermitian](https://en.wikipedia.org/wiki/Hermitian_function), which results in the Fourier transform being real-valued. In the end, we compute these amplitudes with a mixture of random noise and parameters such as wind speed, wind fetch, simulation time, and others.

We also use three cascades to divide the ocean spectrum and generate three displacement maps at different world-space scales. This allows us to capture detail across multiple scales, since for good performance the grid size of the ocean spectrum must be quite small. We chose 512 by 512 waves for each cascade, with scales of 200, 50, and 10 meters. This is $512 * 512 * 3 = 786432$ unique waves in total, although many are low contribution. This leads to a spatial and wavelength sample rate of $10 / 512 \approx 0.02$ meters, which is roughly two centimeters. This is the approximate boundary at which gravity waves become capillary waves and surface tension becomes the dominating force. The models we use rely on the fact that gravity dominates, and do not sufficiently account for surface tension.

Next, surface normals are needed for shading. We calculate these from the gradients of the displacement, instead of a numerical method like finite differences. We compute the tangent, bitangent, and normal from the final displaced ocean surface vertex position $D(\vec r,t)$ as follows:

$$
\begin{align*}
\vec D(\vec r,t) &= (x+D_x,y+D_y,z+D_z) \\
\vec T &= \frac{d}{dx}\vec D(\vec r,t) = \left(1+\frac{d}{dx}D_x,\frac{d}{dx}D_y,\frac{d}{dx}D_z\right)\\
\vec B &= \frac{d}{dz}\vec D(\vec r,t) = \left(\frac{d}{dz}D_x,\frac{d}{dz}D_y,1+\frac{d}{dz}D_z\right)\\
\vec N &= \vec T \times \vec B
\end{align*}
$$

The discrete fourier transform is a sum, so the derivative distributes over it. Thus we can instead take the inverse fourier of the per-term partial derivatives. We need six partial derivatives obtained by distributing $\frac{d}{dx}$ and $\frac{d}{dz}$ to the three components of the displacement. It can be shown that the mixed partials $\frac{d}{dx}D_z$ and $\frac{d}{dz}D_x$ are equal, so we only need to compute five extra IFFTs for eight in total.

We can improve the performance further. Since the displacement is real-valued, we can pack two IFFTs into one by summing two streams of input data while multiplying one by $i$. The discrete fourier transform is a linear sum, so multiplying the inputs by a scalar multiplies the output by that same scalar. For example:

$$
\tilde{h}_x(k,t)+i\tilde{h}_z(k,t)\xmapsto{IFFT}D_x+iD_z
$$

From this we can easily extract $D_x$ and $D_z$ since they will be in separate vector components. Furthermore, since textures have four components, we can pack two IFFTs into one set of dispatches via concatenation. See `computeRealizedAmplitude` of [`fourier_waves.wgsl`](../../shaders/sky-sea/ocean/fourier_waves.wgsl#L354) for how we pack the spectrum data.

In the end, we write the displacement, partial derivatives, and surface jacobian into arrays of maps that are sampled in the vertex and fragment shaders to rasterize the final ocean surface mesh into the GBuffer.

### Atmosphere

The technique for the atmosphere is based heavily on a 2020 paper by SÃ©bastien Hillaire titled "A scalable and production ready sky and atmosphere rendering technique" (see [[6]](#hillaire-2020)). The final output of the atmosphere step is a set of four lookup tables: the transmittance LUT, skyview LUT, multiscatter LUT, and aerial perspective LUT. See Hillaire's paper for all of the equations, but here is a brief overview of each resource.

The atmosphere is modelled as a medium that continuously scatters light. We consider the Mie and Rayleigh models of scattering, alongside the scattering due to ozone. See `computeLuminanceScatteringIntegral` in [`atmosphere_raymarch.inc.wgsl`](../../shaders/sky-sea/atmosphere/atmosphere_raymarch.inc.wgsl) for the raymarching algorithm used to evaluate the scattering integral, which is used in multiple places. Scattering and absorption factors determine the [optical depth](https://en.wikipedia.org/wiki/Optical_depth), see [`GlobalUBO.ts`](./GlobalUBO.ts) for where these parameters are set, and `sampleExtinction` in [`atmosphere_common.inc.wgsl`](../../shaders/sky-sea/atmosphere/atmosphere_common.inc.wgsl) for where optical depth is determined. Mie and Rayleigh extinction is modelled with exponential functions that decay with altitude as a parameter, and their maximal influence is right at the planet's surface. Ozone is modelled with a tent function that places its maximal influence much higher in the atmosphere.

The transmittance LUT (see [`transmittance_LUT.wgsl`](../../shaders/sky-sea/atmosphere/transmittance_LUT.wgsl)) measures the spectral transmittance for red, blue, and green along a given view ray, all the way to the edge of the atmosphere. The map is parameterized by altitude and view zenith angle. The transmittance is found by integrating optical depth along a discrete number of steps, where a smaller step size leads to higher accuracy. Sampling the transmittance LUT should be avoided for low altitudes at angles nearly parallel to the surface of the planet, since you run into precision issues. In general, raymarching to determine transmittance is better but the LUT is faster. By default, we recompute transmittance when raymarching luminance, which we only do in our lookup tables and not in the final composition pass.

The multiscatter LUT (see [`multiscatter_LUT.wgsl`](../../shaders/sky-sea/atmosphere/multiscatter_LUT.wgsl)) contains a map of the total incoming luminance from all directions due to higher orders of scattering, i.e. light that bounces a few times. The map is parameterized by altitude and light zenith angle. This means that only the the current time of day and the viewer's altitude varies multiscattering, not longitude and latitude. Higher orders of scattered light are roughly correlated, which means their sum can be accurately estimated by a geometric series, making the multiscattering LUT much easier to calculate. Even so, this calculation is the most expensive of our LUTs.

The skyview LUT (see [`skyview_LUT.wgsl`](../../shaders/sky-sea/atmosphere/skyview_LUT.wgsl)) contains a map of the total incoming luminance along a ray from the camera's position. Each texel is just a single evaluation of the scattering integral for light reaching the observer. It contains a projection of the full spherical view, and is parameterized by azimuthal and zenith viewing angles from the camera's position. Thus, it only needs to be recomputed if the atmosphere parameters change, or the camera moves, but not as the camera rotates. This LUT provides a short computation path for unobstructed views of the sky, and can be a relatively low resolution due to the low signal frequency of the luminance.

The aerial perspective LUT (see [`aerial_perspective_LUT.wgsl`](../../shaders/sky-sea/atmosphere/aerial_perspective_LUT.wgsl)) is used for determining the [aerial perspective](https://en.wikipedia.org/wiki/Aerial_perspective) for surface geometry, in which distant objects are gradually obscured by the scattering effects of the atmosphere. The aerial perspective LUT is fit to the camera frustum, and is an array with multiple depth slices. By default the texture is 32 texels on all dimensions, with a range of one kilometer per depth-slice. Aerial perspective is low frequency, so this small resolution is suitable. The aerial perspective LUT is very similar to the skyview LUT, just with different parameterization.

Each frame, we only need to recompute the skyview LUT and aerial perspective LUT, since those two are the ones that depend on time of day and viewer position. The transmittance LUT and multiscatter LUT only depend on the atmosphere's conditions (excluding sun position), and thus only need to be generated initially and when e.g. scattering coefficients are changed.

### Composition

See [`atmosphere_camera.wgsl`](../../shaders/sky-sea/atmosphere_camera.wgsl) and [`AtmosphereCamera.ts`](./AtmosphereCamera.ts) for where the rendering of the final scene occurs.

We start with the ocean surface in the GBuffer. For each texel, we decide if it contains surface geometry based on if the world distance was set in the GBuffer. If it contains the sky, we sample the skyview LUT and layer a sun disk on top. If it contains geometry, we assume it is water and shade it accordingly. The model we use for shading the ocean pixels is quite simple and inaccurate, since the focus of this project was on the ocean simulation and general integration with the atmosphere.

We wish to compute the luminance incident to the camera for every pixel. Consider:

$$
\begin{align*}
\vec C &: \text{Camera position} \\
\vec P &: \text{Surface position} \\
\vec L &: \text{Light direction of travel} \\
\vec V &\coloneqq \operatorname{normalize}(C - P) \text{, View direction} \\
\vec H &\coloneqq \operatorname{normalize(\vec L + \vec V)} \text{, Halfway vector} \\
\vec N &: \text{Surface normal} \\
R(\vec v,\vec n) &: \text{Fresnel factor for direction $\vec v$ and surface normal $\vec n$} \\
L(\vec p,\vec v) &: \text{Luminance in direction $\vec v$ from point $\vec p$, incoming or outgoing} \\
T(\vec x, \vec y) &: \text{Transmittance between points $\vec x$ and $\vec y$} \\
\operatorname{BRDF}(\vec a, \vec b) &: \text{BRDF for outgoing vectors $\vec a$ and $\vec b$} \\
\omega &: \text{Solid angle in steradians} \\
\end{align*}
$$

We wish to compute $L_{camera}(\vec C, -\vec V)$, the incoming luminance for the camera in the direction of the surface point. We continue as follows:

$$
\begin{align*}
L_{camera}(\vec C, -\vec V) &= T(\vec C, \vec P)*L_{surface}(\vec P, \vec V) \\
L_{surface}(\vec P,\vec V) &= L_{sky}+L_{sun} \\
\end{align*}
$$

Estimating the luminance from the sky dome accurately is difficult, see [[3]](#bruneton-2010) for some techniques addressing this. We take a simple approach and split into separate specular and diffuse terms, with disjoint sampling strategies. The samples chosen are arbitrary and imprecise, but they work well enough for a visually pleasing scene.

$$
\begin{align*}
L_{sky} &= \omega_s*L_{in,sky}(\vec v_s)*\operatorname{BRDF_s}(\vec v_s,\vec V)R_s  \\
&+ \omega_d*L_{in,sky}(\vec v_d)*\operatorname{BRDF_d}(\vec v_d,\vec V)R_d \\
\omega_s &= \frac{4\pi}{200}\\
\vec v_{s} &= \operatorname{reflect(\vec V, \vec N)} \\
R_s &= R(\vec v_s, \vec N) \\
\omega_d &= 2\pi \\
\vec v_{d} &= \operatorname{normalize(\vec L+(0,1,0))} \\
R_d &= 1 - R(\vec v_d, \operatorname{normalize}(\vec v_d+\vec L)) \\
\end{align*}
$$

$L_{in,sky}$ is sampled from the skyview LUT.

For $BRDF_s$, we use a specular microfacet BRDF. To determine the sample, we assume the maximal impact sample is in the direction of a perfect reflection. At sunset, the luminance varies by about 20 times from minimum to maximum. At noon, this variation is only 2 times. Thus, since the specular BRDF has a sharp falloff, this assumption is reasonably accurate. So our choice of $\vec v_s$ in the direction of a perfect reflection is suitable.

For $BRDF_d$, we use a diffuse lambertian BRDF. To determine the sample, we assume a sample direction that is halfway between the light and world up roughly approximates the mean luminance from the sky. This is because, as stated earlier for the specular sample, the sky luminance does not vary much. So our choice of $\vec v_d$ is suitable.

We compute the sun luminance as follows:

$$
\begin{align*}
S(\vec p, \vec l) \in [0,1] &: \text{Sun visibility at surface position $\vec p$ and light direction $\vec l$} \\
\vec A(\vec o, \vec d) &: \text{Atmosphere boundary, from $\vec o$ in direction $\vec d$ } \\
L_{sun} &= S(\vec C, \vec L) * E_{sun} * BRDF_{sun} \\
E_{sun} &= T(\vec P, \vec A(\vec C, \vec L))\\

BRDF_{sun} &= \operatorname{lerp}(BRDF_s(\vec L, \vec V),BRDF_d(\vec L, \vec V),\vec R(\vec L,\vec H))\\
\end{align*}
$$

The visibility of the sun varies as the sun dips below the horizon, and thus so does the incoming luminance.

An important fact that is not obvious is that the final luminance we are computing is linear in terms of the illuminance hitting the atmospheric boundary. We can fix this illuminance to be 1, then multiply at the end by an arbitrary solar strength factor that combines the sun's apparent solid angle and luminous intensity. This is why we compute $E_{sun}$ as just the transmittance from the surface to the sun.

This also means that $L_{camera}$ is not luminance but actually a linear transfer factor that converts from illuminance at the atmospheric boundary from the sun, to luminance hitting the camera in the view direction.

After computing $L_{camera}$, we multiply it by the arbitrary strength and color of the sun to get the final HDR luminance. We convert to sRGB with the ACES tonemapping function, then present to the screen.

## Further Work

- Improve the tiling to make it aperiodic, see [[7]](#lutz-2024) for an option for the technique
- Better level-of-detail for the distant ocean
  - Currently displacement maps are sampled with mipmaps and anisotropic filtering, this still leads to instability when the camera moves due to the unstable vertex positions of the ocean surface mesh
  - The distant ocean needs to be modelled in a way that captures the variance of the slopes, potentially see the BRDF technique by Bruneton et. al. in [[3]](#bruneton-2010)
- The ocean surface needs a more realistic lighting model. It is currently lacking:
  - Light transmitting through waves
  - Secondary bounces
  - Refraction
  - General accuracy
- More realistic foam and spray
  - Handle loops in wave crests
- Underwater camera
- Clouds
- Volumetric shadows
- Moon
- Smooth the sun-disk

## Sources

<a class="citation" id="tessendorf-2001">[1]</a>
Jerry Tessendorf. 2001.
Simulating Ocean Water.
SIG-GRAPH'99 Course Note.
<https://jtessen.people.clemson.edu/reports/papers_files/coursenotes2002.pdf>

<a class="citation" id="bruneton-2008">[2]</a>
Eric Bruneton, Fabrice Neyret. 2008.
Precomputed Atmospheric Scattering.
Computer Graphics Forum, Special Issue: Proceedings of the 19th Eurographics Symposium on Rendering 2008, 27(4), pp.1079-1086.
<https://doi.org/10.1111/j.1467-8659.2008.01245.x> . <https://inria.hal.science/inria-00288758>

<a class="citation" id="bruneton-2010">[3]</a>
Eric Bruneton, Fabrice Neyret, Nicolas Holzschuch. 2010.
Real-time Realistic Ocean Lighting using Seamless Transitions from Geometry to BRDF.
Computer Graphics Forum, 29(2), pp. 487-496.
<https://doi.org/10.1111/j.1467-8659.2009.01618.x> . <https://inria.hal.science/inria-00443630v3>

<a class="citation" id="flugge-2017">[4]</a>
Fynn-Jorin FlÃ¼gge. 2017.
Realtime GPGPU FFT ocean water simulation.
<https://doi.org/10.15480/882.1436>

<a class="citation" id="gamper-2018">[5]</a>
Thomas Gamper. 2018.
Ocean surface generation and rendering [Diploma Thesis, Technische UniversitÃ¤t Wien].
reposiTUm.
<https://doi.org/10.34726/hss.2018.57880>

<a class="citation" id="hillaire-2020">[6]</a>
SÃ©bastien Hillaire. 2020.
A scalable and production ready sky and atmosphere rendering technique.
Computer Graphics Forum, 39(4), pp. 13â22.
<https://doi.org/10.1111/cgf.14050>

<a class="citation" id="lutz-2024">[7]</a>
Nicolas Lutz, Arnaud Schoentgen, and Guillaume Gilet. 2024.
Fast orientable aperiodic ocean synthesis using tiling and blending.
Proc. ACM Comput. Graph. Interact. Tech. 7, 3, Article 49 (August 2024), 22 pages.
<https://doi.org/10.1145/3675388>
