# WebGPU Sky and Sea

## Abbreviations

- FT - fourier transform
- FFT - fast fourier transform
- IFFT - inverse fast fourier transform
- LUT - lookup table

## Features

- Ocean
  - Gerstner waves, also known as trochoid waves
  - FFT acceleration for millions of waves instead of a few dozen
  - Gravity-waves with directional spreading and JONSWAP spectrum
  - Parameterized wind
  - Cascades that subdivide full frequency spectrum for more detail
  - Foam that forms and dissipates based on surface jacobian
  - Ocean surface mesh projected from screen-space to world-space to minimize offscreen geometry
  - Rasterization into GBuffer with per vertex displacement and per fragment normals
- Atmosphere
  - Dynamic time of day
  - Scattering integral evaluated via raymarching
  - Rayleigh, Mie, and Ozone scattering
  - Multiple scattering and aerial perspective
  - Composited sun disk
  - Lookup tables for transmittance, view of the sky, multiscattering, and aerial perspective
- Ocean surface shading includes luminance from the sky and sun
- Able to display most intermediate resources (such as the complex amplitudes of the ocean FFT spectrum) to the screen, see "Render Output > Render Output" in the controls

## About

The renderer works by generating the atmosphere and ocean, before compositing them in a final compute pass.

### Ocean

The ocean surface is generated as a mesh whose vertices are generated on the GPU and then rasterized into a GBuffer with a graphics pass. The GBuffer contains textures for color, world-space normals, and depth. World-space distance is packed into the alpha channel of the color texture, and foam strength is packed into the alpha channel of the normal texture.

See [`wave_surface_displacement.wgsl`](../../shaders/sky-sea/ocean/wave_surface_displacement.wgsl) and [`WaveDisplacement.ts`](./ocean/WaveDisplacement.ts) for the implementation of the ocean surface mesh. The mesh vertices are first generated by projecting from the screen-space of the scene camera onto the spherical planet surface. This gives us a subdivided quad that is the rough intersection of the camera frustum with the unperturbed ocean, with near constant screen-space vertex density. Even though the eventual displacement due to waves warps the density, this means we are dedicating roughly the same amount of computation time for every rasterized pixel.

Once we have these initial vertices, we displace each vertex to simulate waves. The technique for calculating the displacements and surface normals are based primarily on the 2001 paper "Simulating Ocean Water" by Jerry Tessendorf, see [[1]](#tessendorf-2001) for all the details. The master's thesis "Ocean surface generation and rendering" by Thomas Gamper [[5]](#gamper-2018) provides an excellent, more modern survey of ocean simulation. We'll give a brief overview here.

The displacements are three dimensional, since that leads to more realistic trochoid shaped waves that roll like real ocean waves. The simplest technique for determining this per-vertex displacement is by evaluating the sine/cosine functions for a fixed list of waves in the vertex shader. However, unless you want a stylistic look, for hundreds of waves the result is quite unrealistic. You could use millions of waves, but the `O(n)` scaling leads to poor performance.

Luckily, we can apply the Fast Fourier Transform (FFT) to knock down the complexity, since a fourier transform is the desired sum of a bunch of waves. See [`fourier_waves.wgsl`](../../shaders/sky-sea/ocean/fourier_waves.wgsl) and [`FourierWaves.ts`](./ocean/FourierWaves.ts) for the implementation as it relates to the ocean, or [`fft.wgsl`](../../shaders/sky-sea/util/fft.wgsl) and [`FFT.ts`](./util/FFT.ts) for the general FFT implementation. We reformulate our spectrum of waves from a linear list to a square grid of width `n`, so calculating a single displacement is naively an `O(n^2)` operation. A normal FFT has `O(n * log n)` time complexity instead, allowing for many more waves. Each term of the real-valued displacement `(Dx,Dy,Dz)` requires its own inverse FFT to compute, so we require three inverse FFTs. We save on the three expensive cosine/sine calculations per `n^2` waves that this would otherwise require.

The complex frequency-domain amplitudes used as the input to the inverse FFT are calculated from empirically derived spectrums such as JONSWAP. See Tessendorf and Gamper's papers for more detail, since there are options and some artistic freedom for what to use. In the end, the amplitudes are determined from parameters such as wind speed, wind fetch, simulation time, and a few others.

We also use three cascades to divide the ocean spectrum and generate three displacement maps at different world-space scales. This allows us to capture detail across multiple scales, since for performance the grid size of the ocean spectrum must be quite small. We chose 512 by 512 waves for each cascade, with scales of 200, 50, and 10 meters. This is 786,432 waves in total, although many do not contribute much. This leads to a spatial and wavelength sample rate of roughly two centimeters, which is a rough point at which gravity waves become capillary waves and surface tension becomes the dominating force. The models we use do not account for surface tension, and rely on the fact that gravity dominates.

Surface normals are needed for shading. We calculate these from the gradients of the displacement, instead of using the less accurate finite differences method. The fourier transform is a sum, so the derivative distributes over it. Thus we can instead take the inverse fourier of the per-term partial derivatives. Our displacement has three terms `(Dx,Dy,Dz)`, thus we need six partial derivatives obtained by distributing `d/dx` and `d/dz`. It can be shown that the mixed partials `dDz/dx` and `dDx/dz` are equal, so we only need to compute five extra IFFTs for eight in total.

We can improve the performance further. The displacement is a real-valued function, so we can actually pack two IFFTs into one by summing the input data while multiplying one by `i`. The fourier transform is a linear sum, so multiplying the inputs by `i` multiplies the output by `i`. For example, if `f(k)` and `g(k)` are two complex valued functions such that `F(k)` and `G(k)` are the real valued results of the fourier transform, then the fourier transform of `f(k) + i*g(k)` will be `F(k) + i*G(k)` and we can easily extract `F(k)` and `G(k)`. See the discussion about Hermitian spectrums by Gamper. Furthermore, since complex numbers are modelled as vectors with two components, and RGBA textures have four components, we can pack two IFFTs into one set of dispatches. See `computeRealizedAmplitude` in [`fourier_waves.wgsl`](../../shaders/sky-sea/ocean/fourier_waves.wgsl) for how we pack the spectrum data.

In the end, we compute the displacement, partial derivatives, and surface jacobian into arrays of maps that are sampled in the vertex and fragment shaders to rasterize the final ocean surface mesh into the GBuffer.

### Atmosphere

The technique for the atmosphere is based heavily on a 2020 paper by Sébastien Hillaire titled "A scalable and production ready sky and atmosphere rendering technique" (see [[6]](#hillaire-2020)). The final output of the atmosphere step is a set of four lookup tables: the transmittance LUT, skyview LUT, multiscatter LUT, and aerial perspective LUT. See Hillaire's paper for more detail, but here is a brief overview.

The atmosphere is modelled as a medium that continuously scatters light. We consider the Mie and Rayleigh models of scattering, alongside the scattering due to ozone. See `computeLuminanceScatteringIntegral` in [`atmosphere_raymarch.inc.wgsl`](../../shaders/sky-sea/atmosphere/atmosphere_raymarch.inc.wgsl) for the raymarching algorithm used to evaluate the scattering integral, which is used in multiple places. Scattering and absorption factors determine the [optical depth](https://en.wikipedia.org/wiki/Optical_depth), see [`GlobalUBO.ts`](./GlobalUBO.ts) for where these parameters are set, and `sampleExtinction` in [`atmosphere_common.inc.wgsl`](../../shaders/sky-sea/atmosphere/atmosphere_common.inc.wgsl) for where optical depth is determined.

The transmittance LUT (see [`transmittance_LUT.wgsl`](../../shaders/sky-sea/atmosphere/transmittance_LUT.wgsl)) measures the spectral transmittance for red, blue, and green along a given view ray, all the way to the edge of the atmosphere. The map is parameterized by altitude and view zenith angle. The transmittance is found by integrating optical depth along a discrete number of steps, where a smaller step size leads to higher accuracy. Sampling the transmittance LUT should be avoided for low altitudes at angles nearly parallel to the surface of the planet, since you run into precision issues. In general, raymarching to determine transmittance is better but the LUT is faster.

The multiscatter LUT (see [`multiscatter_LUT.wgsl`](../../shaders/sky-sea/atmosphere/multiscatter_LUT.wgsl)) contains a map of the total incoming luminance from all directions due to higher orders of scattering, so light that bounces around a few times. The map is parameterized by altitude and light zenith angle. Higher orders of scattered light are roughly correlated, which means they can be accurately estimated by a geometric sequence and their sum a geometric series. We compute the multiscattering by summing the primary scattered light in hundreds of directions,

The skyview LUT (see [`skyview_LUT.wgsl`](../../shaders/sky-sea/atmosphere/skyview_LUT.wgsl)) contains a map of the total incoming luminance along a ray from the camera's position. Each texel is just a single evaluation of the scattering integral for light reaching the observer. It contains a projection of the full spherical view, and is parameterized by azimuthal and zenith viewing angles from the camera's position. Thus, it only needs to be recomputed if the atmosphere parameters change, or the camera moves, but not as the camera rotates. This LUT provides a short path for unobstructed views of the sky, and can be a relatively low resolution due to the variation of the sky's luminance being low frequency.

The aerial perspective LUT (see [`aerial_perspective_LUT.wgsl`](../../shaders/sky-sea/atmosphere/aerial_perspective_LUT.wgsl)) is used for determining the [aerial perspective](https://en.wikipedia.org/wiki/Aerial_perspective) for surface geometry, in which distant objects are gradually obscured by the scattering effects of the atmosphere. The aerial perspective LUT is fit to the camera frustum with multiple depth slices. By default the texture is 32 texels on all dimensions, with a range of one kilometer per slice. Aerial perspective is low frequency, so a small resolution is suitable.

### Composition

See [`atmosphere_camera.wgsl`](../../shaders/sky-sea/atmosphere_camera.wgsl) and [`AtmosphereCamera.ts`](./AtmosphereCamera.ts) for where the rendering of the final scene occurs.

We start with the ocean surface in the GBuffer. For each texel, we decide if it contains surface geometry based on if the world distance was set in the GBuffer. If it contains the sky, we sample the skyview LUT and layer a sun disk on top. If it contains geometry, we assume it is water and shade it according to the indirect sky-dome and direct sunlight. The model we use is quite simple and inaccurate, since the focus of this project was on the ocean simulation and integration with the atmosphere.

Estimating the luminance from the sky dome accurately is difficult, see [[3]](#bruneton-2010) for techniques more accurately addressing this. Instead we take a simpler approach, and estimate the total luminance as a perfect reflection term plus a diffuse term. The perfect reflection is calculated as follows:

```wgsl
let reflection_direction = reflect(normalize(direction), normalize(material.normal));
let sky_luminance = sampleSkyViewLUT(atmosphere, surface_position, reflection_direction);
light_luminance_transfer +=
  transmittance_to_surface
  * sky_luminance
  * computeFresnelPerfectReflection(material, reflection_direction);
```

We utilize the fresnel term and assume all light that is transmitted and eventually scattered by the sea is captured by the diffuse term, which is calculated as follows:

```wgsl
let sky_visible_solid_angle =
  2.0 * PI
  * (0.5 * dot(vec3<f32>(0.0, 1.0, 0.0), material.normal) + 0.5);
let sky_indirect_luminance = sampleSkyViewLUT(
  atmosphere,
  surface_position,
  reflect(-light_direction, vec3<f32>(0.0,1.0,0.0))
);
let sea_luminance =
  diffuseBRDF(material)
  * sky_visible_solid_angle
  * sky_indirect_luminance;
light_luminance_transfer += transmittance_to_surface * sea_luminance;
```

We use a single luminance sample to estimate the illuminance. The solid angle we use assumes that the surface is locally a plane, which obscures the hemisphere sky-dome with a factor of the cosine of the angle between the normal and the world up direction `(0,1,0)`. This is not a great estimate since most of the luminance will be due to scattering interactions deeper in the water, but stylistically it works well enough.

The direct sunlight is modelled with a modified Cook-Torrance model, as follows:

```wgsl
let surface_transmittance_to_sun = sampleTransmittanceLUT_Ray(
  transmittance_lut,
      lut_sampler,
  atmosphere,
  surface_position,
  light_direction
);
let light_luminance = surface_transmittance_to_sun
  * sunFractionOfRadianceVisible(atmosphere, light, surface_position, light_direction);
light_luminance_transfer +=
  transmittance_to_surface
  * light_luminance
  * mix(
    specularBRDF(material, light_direction, -direction),
    diffuseBRDF(material),
    computeFresnelMicrofacet(material, light_direction, -direction)
  );
```

Note the specular and diffuse BRDFs, interpolated by a fresnel term. The function `sunFractionOfRadianceVisible` includes the limb darkening of the sun, as well as considering what percent of the sun is visible above the horizon.

After computing the final luminance of the texel, we convert HDR physically based luminance values to sRGB color values with the ACES tonemapping function. We then present the result to the screen.

## Further Work

- Improve the tiling to make it aperiodic, see [[7]](#lutz-2024) for an option for the technique
- Better level-of-detail for the distant ocean
  - Currently displacement maps are sampled with mipmaps and anisotropic filtering, this still leads to instability when the camera moves due to the unstable vertex positions of the ocean surface mesh
  - The distant ocean needs to be modelled in a way that captures the variance of the slopes, potentially see the BRDF technique by Bruneton et. al. in [[3]](#bruneton-2010)
- The ocean surface needs a more realistic lighting model. It is currently lacking:
  - Light transmitting through waves
  - Secondary bounces
  - Refraction
  - General accuracy
- More realistic foam and spray
  - Handle loops in wave crests
- Underwater camera
- Clouds
- Volumetric shadows
- Moon
- Smooth the sun-disk

## Sources

<a id="tessendorf-2001">[1]</a>
Jerry Tessendorf. 2001.
Simulating Ocean Water.
SIG-GRAPH'99 Course Note.
<https://jtessen.people.clemson.edu/reports/papers_files/coursenotes2002.pdf>

<a id="bruneton-2008">[2]</a>
Eric Bruneton, Fabrice Neyret. 2008.
Precomputed Atmospheric Scattering.
Computer Graphics Forum, Special Issue: Proceedings of the 19th Eurographics Symposium on Rendering 2008, 27(4), pp.1079-1086.
<https://doi.org/10.1111/j.1467-8659.2008.01245.x> . <https://inria.hal.science/inria-00288758>

<a id="bruneton-2010">[3]</a>
Eric Bruneton, Fabrice Neyret, Nicolas Holzschuch. 2010.
Real-time Realistic Ocean Lighting using Seamless Transitions from Geometry to BRDF.
Computer Graphics Forum, 29(2), pp. 487-496.
<https://doi.org/10.1111/j.1467-8659.2009.01618.x> . <https://inria.hal.science/inria-00443630v3>

<a id="flugge-2017">[4]</a>
Fynn-Jorin Flügge. 2017.
Realtime GPGPU FFT ocean water simulation.
<https://doi.org/10.15480/882.1436>

<a id="gamper-2018">[5]</a>
Thomas Gamper. 2018.
Ocean surface generation and rendering [Diploma Thesis, Technische Universität Wien].
reposiTUm.
<https://doi.org/10.34726/hss.2018.57880>

<a id="hillaire-2020">[6]</a>
Sébastien Hillaire. 2020.
A scalable and production ready sky and atmosphere rendering technique.
Computer Graphics Forum, 39(4), pp. 13–22.
<https://doi.org/10.1111/cgf.14050>

<a id="lutz-2024">[7]</a>
Nicolas Lutz, Arnaud Schoentgen, and Guillaume Gilet. 2024.
Fast orientable aperiodic ocean synthesis using tiling and blending.
Proc. ACM Comput. Graph. Interact. Tech. 7, 3, Article 49 (August 2024), 22 pages.
<https://doi.org/10.1145/3675388>
